{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder \\\n",
        "      .appName(\"Basics\") \\\n",
        "      .getOrCreate()"
      ],
      "metadata": {
        "id": "QRSFhPr6S11k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P413XatBSvUK"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"O001\",\"Amit\",\"Hyderabad\",\"Spice Hub\",\"Indian\",450,35,\"UPI\",\"Delivered\"),\n",
        "    (\"O002\",\"Neha\",\"Bangalore\",\"Pizza Town\",\"Italian\",650,40,\"Card\",\"Delivered\"),\n",
        "    (\"O003\",\"Rahul\",\"Delhi\",\"Burger Zone\",\"American\",520,30,\"Cash\",\"Delivered\"),\n",
        "    (\"O004\",\"Pooja\",\"Mumbai\",\"Sushi Bar\",\"Japanese\",1200,55,\"UPI\",\"Cancelled\"),\n",
        "    (\"O005\",\"Arjun\",\"Chennai\",\"Curry Leaf\",\"Indian\",380,28,\"UPI\",\"Delivered\"),\n",
        "    (\"O006\",\"Sneha\",\"Hyderabad\",\"Pasta Street\",\"Italian\",700,45,\"Card\",\"Delivered\"),\n",
        "    (\"O007\",\"Karan\",\"Delhi\",\"Taco Bell\",\"Mexican\",540,33,\"UPI\",\"Delivered\"),\n",
        "    (\"O008\",\"Riya\",\"Bangalore\",\"Dragon Bowl\",\"Chinese\",600,38,\"Wallet\",\"Delivered\"),\n",
        "    (\"O009\",\"Vikas\",\"Mumbai\",\"BBQ Nation\",\"Indian\",1500,60,\"Card\",\"Delivered\"),\n",
        "    (\"O010\",\"Anjali\",\"Chennai\",\"Burger Zone\",\"American\",480,32,\"Cash\",\"Delivered\"),\n",
        "    (\"O011\",\"Farhan\",\"Delhi\",\"Biryani House\",\"Indian\",520,36,\"UPI\",\"Delivered\"),\n",
        "    (\"O012\",\"Megha\",\"Hyderabad\",\"Sushi Bar\",\"Japanese\",1100,58,\"Card\",\"Cancelled\"),\n",
        "    (\"O013\",\"Suresh\",\"Bangalore\",\"Curry Leaf\",\"Indian\",420,29,\"UPI\",\"Delivered\"),\n",
        "    (\"O014\",\"Divya\",\"Mumbai\",\"Pizza Town\",\"Italian\",780,42,\"Wallet\",\"Delivered\"),\n",
        "    (\"O015\",\"Nikhil\",\"Delhi\",\"Pasta Street\",\"Italian\",690,47,\"UPI\",\"Delivered\"),\n",
        "    (\"O016\",\"Kavya\",\"Chennai\",\"Dragon Bowl\",\"Chinese\",560,34,\"UPI\",\"Delivered\"),\n",
        "    (\"O017\",\"Rohit\",\"Hyderabad\",\"BBQ Nation\",\"Indian\",1400,62,\"Card\",\"Delivered\"),\n",
        "    (\"O018\",\"Simran\",\"Bangalore\",\"Burger Zone\",\"American\",510,31,\"Cash\",\"Delivered\"),\n",
        "    (\"O019\",\"Ayesha\",\"Mumbai\",\"Taco Bell\",\"Mexican\",570,35,\"UPI\",\"Delivered\"),\n",
        "    (\"O020\",\"Manish\",\"Delhi\",\"Curry Leaf\",\"Indian\",390,27,\"Wallet\",\"Delivered\"),\n",
        "    (\"O021\",\"Priya\",\"Hyderabad\",\"Pizza Town\",\"Italian\",720,41,\"Card\",\"Delivered\"),\n",
        "    (\"O022\",\"Yash\",\"Chennai\",\"Sushi Bar\",\"Japanese\",1150,57,\"UPI\",\"Delivered\"),\n",
        "    (\"O023\",\"Naina\",\"Bangalore\",\"Pasta Street\",\"Italian\",680,44,\"UPI\",\"Delivered\"),\n",
        "    (\"O024\",\"Sameer\",\"Mumbai\",\"Dragon Bowl\",\"Chinese\",610,39,\"Wallet\",\"Delivered\"),\n",
        "    (\"O025\",\"Ritika\",\"Delhi\",\"Burger Zone\",\"American\",500,30,\"Cash\",\"Delivered\"),\n",
        "    (\"O026\",\"Gopal\",\"Hyderabad\",\"Curry Leaf\",\"Indian\",410,28,\"UPI\",\"Delivered\"),\n",
        "    (\"O027\",\"Tina\",\"Bangalore\",\"Pizza Town\",\"Italian\",760,43,\"Card\",\"Delivered\"),\n",
        "    (\"O028\",\"Irfan\",\"Mumbai\",\"BBQ Nation\",\"Indian\",1550,65,\"Card\",\"Delivered\"),\n",
        "    (\"O029\",\"Sahil\",\"Chennai\",\"Taco Bell\",\"Mexican\",590,37,\"UPI\",\"Delivered\"),\n",
        "    (\"O030\",\"Lavanya\",\"Delhi\",\"Dragon Bowl\",\"Chinese\",630,40,\"Wallet\",\"Delivered\"),\n",
        "    (\"O031\",\"Deepak\",\"Hyderabad\",\"Burger Zone\",\"American\",520,33,\"Cash\",\"Delivered\"),\n",
        "    (\"O032\",\"Shweta\",\"Bangalore\",\"Curry Leaf\",\"Indian\",450,31,\"UPI\",\"Delivered\"),\n",
        "    (\"O033\",\"Aman\",\"Mumbai\",\"Pizza Town\",\"Italian\",810,46,\"Card\",\"Delivered\"),\n",
        "    (\"O034\",\"Rekha\",\"Chennai\",\"Pasta Street\",\"Italian\",700,45,\"UPI\",\"Delivered\"),\n",
        "    (\"O035\",\"Zubin\",\"Delhi\",\"BBQ Nation\",\"Indian\",1480,63,\"Card\",\"Delivered\"),\n",
        "    (\"O036\",\"Pallavi\",\"Hyderabad\",\"Dragon Bowl\",\"Chinese\",580,36,\"Wallet\",\"Delivered\"),\n",
        "    (\"O037\",\"Naveen\",\"Bangalore\",\"Taco Bell\",\"Mexican\",560,34,\"UPI\",\"Delivered\"),\n",
        "    (\"O038\",\"Sonia\",\"Mumbai\",\"Sushi Bar\",\"Japanese\",1180,59,\"Card\",\"Delivered\"),\n",
        "    (\"O039\",\"Harish\",\"Chennai\",\"Burger Zone\",\"American\",490,29,\"Cash\",\"Delivered\"),\n",
        "    (\"O040\",\"Kriti\",\"Delhi\",\"Curry Leaf\",\"Indian\",420,26,\"UPI\",\"Delivered\")\n",
        "]\n",
        "\n",
        "columns = [\n",
        "    \"order_id\",\"customer_name\",\"city\",\"restaurant\",\"cuisine\",\n",
        "    \"order_amount\",\"delivery_time_minutes\",\"payment_mode\",\"order_status\"\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write the full dataset to CSV with header enabled."
      ],
      "metadata": {
        "id": "K6EIZyGpTS2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df.write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .option(\"header\", True) \\\n",
        "  .csv(\"orders.csv\")\n",
        "\n",
        "\n",
        "df2 = spark.read \\\n",
        "  .option(\"header\", True) \\\n",
        "  .option(\"inferSchema\", True) \\\n",
        "  .csv(\"orders.csv\")\n",
        "\n",
        "df2.show(30)\n",
        "df2.printSchema()\n"
      ],
      "metadata": {
        "id": "kEXpXC3fS__v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the CSV back and filter:order_amount > 700"
      ],
      "metadata": {
        "id": "jF5v6hxyVjYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df3 = spark.read \\\n",
        "  .option(\"header\", True) \\\n",
        "  .option(\"inferSchema\", True) \\\n",
        "  .csv(\"orders.csv\")\n",
        "new_df=df3.filter(df3.order_amount>700)\n",
        "new_df.show()"
      ],
      "metadata": {
        "id": "QUfjplYeUDBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#From CSV, show only:order_id,city,cuisine,order_amount"
      ],
      "metadata": {
        "id": "oJFquzyzXEeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df4 = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .csv(\"orders.csv\")\n",
        "\n",
        "# Select only the required columns\n",
        "selected_df = df4.select(\"order_id\", \"city\", \"cuisine\", \"order_amount\")\n",
        "\n",
        "# Show the result\n",
        "selected_df.show()\n"
      ],
      "metadata": {
        "id": "zKwBK5SBW8KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sort orders by delivery_time_minutes descending and write result to CSV."
      ],
      "metadata": {
        "id": "Te4LohqLX2r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .csv(\"orders.csv\")\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df5=df4.orderBy(desc(\"delivery_time_minutes\"))\n",
        "df5.coalesce(1) \\\n",
        "  .write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .option(\"header\", True) \\\n",
        "  .csv(\"sort_orders.csv\")"
      ],
      "metadata": {
        "id": "zruYvQMnXu3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write only “Delivered” orders to JSON."
      ],
      "metadata": {
        "id": "KnroCfs3aEWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new=df.filter(df.order_status==\"Delivered\")\n",
        "df_new.write.mode(\"overwrite\").json(\"orders.json\")"
      ],
      "metadata": {
        "id": "9JAfmEffYXWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read JSON and filter:\n",
        "city = \"Mumbai\"\n",
        "payment_mode = \"Card\""
      ],
      "metadata": {
        "id": "u4D0f0NQbNFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2=spark.read.json(\"orders.json\")\n",
        "filter_df=df2.filter((df2.city==\"Mumbai\")&(df2.payment_mode==\"Card\"))\n",
        "filter_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-jDVj47aJut",
        "outputId": "1290fc98-4cf5-4314-d3e1-75e4ef79742a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------------+---------------------+------------+--------+------------+------------+----------+\n",
            "|  city| cuisine|customer_name|delivery_time_minutes|order_amount|order_id|order_status|payment_mode|restaurant|\n",
            "+------+--------+-------------+---------------------+------------+--------+------------+------------+----------+\n",
            "|Mumbai|  Indian|        Irfan|                   65|        1550|    O028|   Delivered|        Card|BBQ Nation|\n",
            "|Mumbai| Italian|         Aman|                   46|         810|    O033|   Delivered|        Card|Pizza Town|\n",
            "|Mumbai|Japanese|        Sonia|                   59|        1180|    O038|   Delivered|        Card| Sushi Bar|\n",
            "|Mumbai|  Indian|        Vikas|                   60|        1500|    O009|   Delivered|        Card|BBQ Nation|\n",
            "+------+--------+-------------+---------------------+------------+--------+------------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add a column:\n",
        "delivery_category\n",
        "Logic:\n",
        "delivery_time_minutes > 45 → \"Late\"\n",
        "else → \"OnTime\"\n",
        "Write output to JSON."
      ],
      "metadata": {
        "id": "9QSddtlkcj5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,when\n",
        "df3 = df2.withColumn(\"delivery_category\", when(col(\"delivery_time_minutes\") > 45, \"Late\").otherwise(\"OnTime\"))\n",
        "\n",
        "df3.show()\n",
        "df3.write.mode(\"overwrite\").json(\"orders_new_col.json\")"
      ],
      "metadata": {
        "id": "cCjTXsHPb69u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Force JSON output to a single partition and observe number of files created."
      ],
      "metadata": {
        "id": "EOyWOJGSdeMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3.coalesce(1)\\\n",
        ".write \\\n",
        ".mode(\"overwrite\")\\\n",
        ".json(\"orders_new_col.json\")"
      ],
      "metadata": {
        "id": "AnpJwZybdGjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert full dataset to Parquet."
      ],
      "metadata": {
        "id": "Dt0RTmLVeRS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"orders_new.parquet\")"
      ],
      "metadata": {
        "id": "qIDr8Pd6dqtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read Parquet and filter:\n",
        "cuisine = \"Indian\"\n",
        "order_amount > 500"
      ],
      "metadata": {
        "id": "Tjh3KgHCgRtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet=spark.read.parquet(\"orders_new.parquet\")\n",
        "filt_df=df_parquet.filter((df_parquet.cuisine==\"Indian\")&(df_parquet.order_amount>500))\n",
        "filt_df.show()"
      ],
      "metadata": {
        "id": "9xIFeVaJfnJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sort Parquet data by order_amount descending and write top 10 orders back to Parquet."
      ],
      "metadata": {
        "id": "aE13nyMSh51M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_parquet=df_parquet.orderBy(desc(\"order_amount\"))\n",
        "top_10_parquet=sorted_parquet.limit(10)\n",
        "top_10_parquet.write.mode(\"overwrite\").parquet(\"top10_orders.parquet\")\n",
        "df_parquet=spark.read.parquet(\"top10_orders.parquet\")\n",
        "df_parquet.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "aBauaqsjgXqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare storage size of:CSVJSONParquet\n",
        "Answer:\n",
        "Which is smallest?\n",
        "Why?"
      ],
      "metadata": {
        "id": "HoQZ5QpejICG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"data_csv\")\n",
        "\n",
        "df.write.mode(\"overwrite\").json(\"data_json\")\n",
        "\n",
        "\n",
        "df.write.mode(\"overwrite\").parquet(\"data_parquet\")\n"
      ],
      "metadata": {
        "id": "dvYErYdPiKoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def get_size(path):\n",
        "    return sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path))\n",
        "\n",
        "print(\"CSV size:\", get_size(\"data_csv\"), \"bytes\")\n",
        "print(\"JSON size:\", get_size(\"data_json\"), \"bytes\")\n",
        "print(\"Parquet size:\", get_size(\"data_parquet\"), \"bytes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJhddp7kjz10",
        "outputId": "564c3490-e5d2-45de-a951-f3f7a82c0eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV size: 2695 bytes\n",
            "JSON size: 8199 bytes\n",
            "Parquet size: 7370 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert:\n",
        "CSV → Parquet\n",
        "JSON → Parquet"
      ],
      "metadata": {
        "id": "imT5VCXPlPr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_json = spark.read.json(\"orders_new_col.json\")\n",
        "\n",
        "df_json.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"orders_new_col.parquet\")\n",
        "df_orders_new_col=spark.read.parquet(\"orders_new_col.parquet\")\n",
        "df_orders_new_col.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "puD3Mnd5j1nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_csv = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .csv(\"sort_orders.csv\")\n",
        "\n",
        "df_csv.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"sort_orders.parquet\")\n",
        "df_sort_orders_parquet=spark.read.parquet(\"sort_orders.parquet\")\n",
        "df_sort_orders_parquet.show()\n"
      ],
      "metadata": {
        "id": "21jH5gpylm9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Parquet and write it back as CSV using delimiter |\n"
      ],
      "metadata": {
        "id": "lai3JeXHnEFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_sort_orders_parquet.write \\\n",
        ".mode(\"overwrite\") \\\n",
        ".option(\"header\", True) \\\n",
        ".option(\"sep\", \"|\")\\\n",
        ".csv(\"newcsv.csv\")\n",
        "\n",
        "\n",
        "df2 = spark.read \\\n",
        "  .option(\"header\", True) \\\n",
        "  .option(\"inferSchema\", True) \\\n",
        "  .option(\"sep\", \"|\") \\\n",
        "  .csv(\"newcsv.csv\")\n",
        "\n",
        "df2.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "--YMkRj1mh3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which cuisine generates the highest order_amount overall?"
      ],
      "metadata": {
        "id": "w_ny0nowoYwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "top_row = df.orderBy(desc(\"order_amount\")).first()\n",
        "print(top_row[\"cuisine\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOC7PxXRnrfJ",
        "outputId": "68679cd3-1876-4b49-d2b0-e9e385c77595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which city has the highest number of orders?"
      ],
      "metadata": {
        "id": "9r1ggnz8pan0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "city_counts = (\n",
        "    df.groupBy(\"city\")\n",
        "      .agg(F.count(\"*\").alias(\"orders\"))\n",
        "      .orderBy(F.desc(\"orders\"))\n",
        ")\n",
        "\n",
        "city_counts.show()\n",
        "\n",
        "top_city = city_counts.first()\n",
        "print(\"Top city:\", top_city[\"city\"], \"Orders:\", top_city[\"orders\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91Sw_uBaonfV",
        "outputId": "10aefe17-f8e4-4af0-9884-c19ead61fa5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|orders|\n",
            "+---------+------+\n",
            "|    Delhi|     9|\n",
            "|Bangalore|     8|\n",
            "|   Mumbai|     8|\n",
            "|Hyderabad|     8|\n",
            "|  Chennai|     7|\n",
            "+---------+------+\n",
            "\n",
            "Top city: Delhi Orders: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which payment mode is most frequently used?"
      ],
      "metadata": {
        "id": "KhVs5Wptu2dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "payment_mode = (\n",
        "    df.groupBy(\"Payment_mode\")\n",
        "      .agg(F.count(\"*\").alias(\"mode\"))\n",
        "      .orderBy(F.desc(\"mode\"))\n",
        ")\n",
        "\n",
        "payment_mode.show()\n",
        "\n",
        "first= payment_mode.first()\n",
        "print(first['Payment_mode'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II_keuviuw7a",
        "outputId": "f52cd0dd-6f1b-42a7-f6da-21798c5a83f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----+\n",
            "|Payment_mode|mode|\n",
            "+------------+----+\n",
            "|         UPI|  17|\n",
            "|        Card|  11|\n",
            "|        Cash|   6|\n",
            "|      Wallet|   6|\n",
            "+------------+----+\n",
            "\n",
            "UPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Why is Parquet the preferred format for analytics platforms like Databricks and BigQuery?\n"
      ],
      "metadata": {
        "id": "sTfWCQ5KvkZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parquet is preferred because it’s fast, efficient, and smart:\n",
        "\n",
        "Columnar format → Reads only the columns you need, speeding up queries.\n",
        "Compression & encoding → Much smaller files than CSV/JSON.\n",
        "Schema built-in → No guessing data types, supports complex structures.\n",
        "Optimized for big data → Enables parallel processing and skips irrelevant data (predicate pushdown).\n",
        "Lower cost → Less storage and faster queries mean cheaper analytics."
      ],
      "metadata": {
        "id": "IXwrZuhrvmjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "Repartition the dataset into 4 partitions and write to Parquet."
      ],
      "metadata": {
        "id": "bLRg8P79wGMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_repartitioned = df.repartition(4)\n",
        "\n",
        "df_repartitioned.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"4part.parquet\")\n"
      ],
      "metadata": {
        "id": "kAvhqZ8NvToK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a report dataset containing:\n",
        "city\n",
        "total_orders\n",
        "total_revenue\n",
        "Write it to Parquet."
      ],
      "metadata": {
        "id": "zzfNAW3uwsAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "df_report = df.withColumn(\"order_amount\", F.col(\"order_amount\").cast(\"double\"))\n",
        "\n",
        "report_df = (\n",
        "\n",
        "    df_report.groupBy(\"city\")\n",
        "    .agg(\n",
        "        F.countDistinct(\"order_id\").alias(\"total_orders\"),\n",
        "        F.sum(\"order_amount\").alias(\"total_revenue\")\n",
        "    )\n",
        "    .orderBy(F.desc(\"total_orders\"))\n",
        ")\n",
        "\n",
        "report_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKIe3t1MwfTv",
        "outputId": "71b38bb3-e178-4f2d-df4e-5084687d0a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+-------------+\n",
            "|     city|total_orders|total_revenue|\n",
            "+---------+------------+-------------+\n",
            "|    Delhi|           9|       5690.0|\n",
            "|Bangalore|           8|       4630.0|\n",
            "|   Mumbai|           8|       8200.0|\n",
            "|Hyderabad|           8|       5880.0|\n",
            "|  Chennai|           7|       4350.0|\n",
            "+---------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"order_report.parquet\")"
      ],
      "metadata": {
        "id": "fM-Gv1Zh1qRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_12=spark.read.parquet(\"order_report.parquet\")\n",
        "df_12.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL_UJCUu2AcV",
        "outputId": "051b5483-236a-46d8-dca6-6bba0e9c6bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+-------------+\n",
            "|     city|total_orders|total_revenue|\n",
            "+---------+------------+-------------+\n",
            "|    Delhi|           9|       5690.0|\n",
            "|Bangalore|           8|       4630.0|\n",
            "|   Mumbai|           8|       8200.0|\n",
            "|Hyderabad|           8|       5880.0|\n",
            "|  Chennai|           7|       4350.0|\n",
            "+---------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZAL-Pyf2K3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}